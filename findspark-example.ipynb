{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-35e57cc8ebe2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/findspark.pyc\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mspark_home\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpython_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/findspark.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mspark_home\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         raise ValueError(\"Couldn't find Spark, make sure SPARK_HOME env is set\"\n\u001b[0m\u001b[0;32m     33\u001b[0m                          \" or Spark is in an expected location (e.g. from homebrew installation).\")\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('spark://192.168.17.1:7077')\n",
    "conf.setAppName('mnist_logistic_regression')\n",
    "sc = SparkContext(conf=conf)\n",
    "print(\"SparkContext created.\")\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping...\n",
      "Reducing...\n",
      "Pi is roughly 3.142818\n",
      "--- 16.6654689312 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import random as ran\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sample(p):\n",
    "    x, y = ran.random(), ran.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "NUM_SAMPLES = 10*1000*1000\n",
    "\n",
    "print(\"Mapping...\")\n",
    "mappedOutput = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample)\n",
    "\n",
    "print(\"Reducing...\")\n",
    "count = mappedOutput.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.901585703499\n",
      "Recall = 0.901585703499\n",
      "F1 Score = 0.901585703499\n",
      "Class 0.0 precision = 0.927223719677\n",
      "Class 0.0 recall = 0.939890710383\n",
      "Class 0.0 F1 Measure = 0.933514246947\n",
      "Class 1.0 precision = 0.952277657267\n",
      "Class 1.0 recall = 0.960612691466\n",
      "Class 1.0 F1 Measure = 0.956427015251\n",
      "Class 2.0 precision = 0.928205128205\n",
      "Class 2.0 recall = 0.868105515588\n",
      "Class 2.0 F1 Measure = 0.897149938042\n",
      "Class 3.0 precision = 0.880893300248\n",
      "Class 3.0 recall = 0.898734177215\n",
      "Class 3.0 F1 Measure = 0.889724310777\n",
      "Class 4.0 precision = 0.899244332494\n",
      "Class 4.0 recall = 0.934554973822\n",
      "Class 4.0 F1 Measure = 0.916559691913\n",
      "Class 5.0 precision = 0.874635568513\n",
      "Class 5.0 recall = 0.817438692098\n",
      "Class 5.0 F1 Measure = 0.845070422535\n",
      "Class 6.0 precision = 0.927461139896\n",
      "Class 6.0 recall = 0.934725848564\n",
      "Class 6.0 F1 Measure = 0.931079323797\n",
      "Class 7.0 precision = 0.901265822785\n",
      "Class 7.0 recall = 0.901265822785\n",
      "Class 7.0 F1 Measure = 0.901265822785\n",
      "Class 8.0 precision = 0.829326923077\n",
      "Class 8.0 recall = 0.869017632242\n",
      "Class 8.0 F1 Measure = 0.848708487085\n",
      "Class 9.0 precision = 0.890510948905\n",
      "Class 9.0 recall = 0.884057971014\n",
      "Class 9.0 F1 Measure = 0.887272727273\n",
      "Weighted recall = 0.901585703499\n",
      "Weighted precision = 0.901888670873\n",
      "Weighted F(1) Score = 0.901445807735\n",
      "Weighted F(0.5) Score = 0.90163972513\n",
      "Weighted false positive rate = 0.010879093308\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Load training data in LIBSVM format\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/home/farmer/scripts/mnist_test.libsvm\")\n",
    "\n",
    "# Split data into training (60%) and test (40%)\n",
    "\n",
    "training, test = data.randomSplit([0.6, 0.4], seed=11L)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "\n",
    "model = LogisticRegressionWithLBFGS.train(training, numClasses=10)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "\n",
    "# Statistics by class\n",
    "\n",
    "labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
