{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3c63d68c66c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# IMPORTENT: SPARK_HOME and PYTHONPATH need to be set first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# only needed when run directly with \"spark-submit\"\n",
    "# OR when executing from the python-shell\n",
    "#--------------------------------------------------\n",
    "\n",
    "# IMPORTENT: SPARK_HOME and PYTHONPATH need to be set first\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "# Running it on the cluster\n",
    "#conf.setMaster('spark://192.168.17.1:7077')\n",
    "\n",
    "# Running it locally on master-node with 4 threads\n",
    "conf.setMaster('local[4]')\n",
    "\n",
    "conf.setAppName('mnist_logistic_regression')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "print(\"CONNECTED.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Started at: 15:41:24 ---\n",
      "Load training and test data in LIBSVM format...\n",
      "Data loaded.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"--- Started at: %d:%d:%d ---\" % (now.hour, now.minute, now.second))\n",
    "\n",
    "# Load training data in LIBSVM format\n",
    "# loadLibSVMFile => RDD[LabeledPoint]\n",
    "\n",
    "print(\"Load training and test data in LIBSVM format...\")\n",
    "#training = MLUtils.loadLibSVMFile(sc, \"/home/farmer/scripts/mnist_train_600.libsvm\", False, 784)\n",
    "#test = MLUtils.loadLibSVMFile(sc, \"/home/farmer/scripts/mnist_test_100.libsvm\", False, 784)\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(sc, \"/home/farmer/scripts/mnist_train.libsvm\")\n",
    "\n",
    "print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared.\n"
     ]
    }
   ],
   "source": [
    "# Split data into training (60%) and test (40%)\n",
    "# randomSplit => Array[RDD[T]]\n",
    "\n",
    "training, test = data.randomSplit([0.9, 0.1], seed=11L)\n",
    "#test, unused = test.randomSplit([1.0, 0.0], seed=11L)\n",
    "\n",
    "print(\"Data prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run training algorithm to build the model...\n"
     ]
    }
   ],
   "source": [
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "\n",
    "print(\"Run training algorithm to build the model...\")\n",
    "model = LogisticRegressionWithLBFGS.train(training, numClasses=10)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "\n",
    "print(\"Compute raw scores on the test set...\")\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Statistics by class\n",
    "\n",
    "labels = data.map(lambda lp: lp.label).distinct().collect()\n",
    "for label in sorted(labels):\n",
    "    print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "    print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "    print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "\n",
    "# Weighted stats\n",
    "\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"--- Finished at: %d:%d:%d ---\" % (now.hour, now.minute, now.second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random as ran\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sample(p):\n",
    "    x, y = ran.random(), ran.random()\n",
    "    return 1 if x*x + y*y < 1 else 0\n",
    "\n",
    "NUM_SAMPLES = 10*1000*1000\n",
    "\n",
    "print(\"Mapping...\")\n",
    "mappedOutput = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample)\n",
    "\n",
    "print(\"Reducing...\")\n",
    "count = mappedOutput.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
